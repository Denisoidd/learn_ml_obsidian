#DL 
For example:
*W = np.random.randn(input, output) **/ np.sqrt(input)*** 

* Works well with **tanh**, but bad with [[ReLU]]. However, if we divide but 2 (**np.sqrt(input /2)**) everything works nicely
* With the use of [[Batch Normalisation]] becomes negligible